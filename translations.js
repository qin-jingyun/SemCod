// --- I18n Translation System ---
    var translations = {
        en: {
            pageTitle: "Neural Coding Is Not Always Semantic",
            paperTitle: "Neural Coding Is Not Always Semantic: Toward the Standardized Coding Workflow in Semantic Communications",
            author_1: "Hai-Long Qin<sup>1</sup>",
            author_2: "Jincheng Dai<sup>1,†</sup>",
            author_3: "Sixian Wang<sup>1</sup>",
            author_4: "Xiaoqi Qin<sup>1</sup>",
            author_5: "Shuo Shao<sup>2</sup>",
            author_6: "Kai Niu<sup>1</sup>",
            author_7: "Wenjun Xu<sup>1</sup>",
            author_8: "Ping Zhang<sup>1,†</sup>",
            affiliation1: "<sup>1</sup>Beijing University of Posts and Telecommunications",
            affiliation2: "<sup>2</sup>University of Shanghai for Science and Technology",
            affiliation_corr: "<sup>†</sup>Corresponding Authors",
            venueInfo: "Accepted by IEEE Communications Standards Magazine (COMSTD), 2025",
            btn_paper: "Paper",
            tooltip_cite: "Cite this paper",
            tooltip_lang: "Language",
            nav_tldr: "TL;DR",
            nav_abstract: "Abstract",
            nav_overview: "Motivation",
            nav_methodology: "Methodology",
            nav_framework: "Framework",
            nav_case_study: "Case Study",
            nav_conclusion: "Conclusion",
            tldr_content: "<p>(1) We establish a <b>standardized coding workflow</b> for semantic communication that moves beyond directly transmitting deep features from neural networks and instead conveys compact, context-aware semantic representations. This workflow formalizes semantic coding as a three-step pipeline: <b>tokenization</b>, <b>reorganization</b>, and optional <b>quantization</b>.</p><p>(2) We introduce <b>token reorganization</b>, a novel mechanism that identifies and merges semantically similar tokens based on contextual similarity. This step is key to compressing raw data into a minimal set of meaningful tokens, approaching a <b>one code, one concept</b> ideal that drastically cuts redundancy and simplifies distribution modeling for transmission.</p><p>(3) We demonstrate that our workflow significantly enhances communication efficiency and robustness. In wireless image transmission, our method uses just <b>10–30 semantic tokens</b> to achieve more realistic reconstructions (lower FID) and higher fidelity (higher PSNR) than traditional neural coding baselines, especially at low data rates.</p><p>(4) We present a flexible, <b>decoupled architecture</b> where general semantic encoding at transmitter is separate from task-specific decoding at receiver. This allows the same semantic representation to be used for diverse applications, enabling high-fidelity content reconstruction for human-centric tasks and efficient decision-making for machine-centric tasks. The receiver only needs to incorporate task-specific conditions into the decoding process to regularize the general semantic representation, thereby producing results aligned with the communication intent.</p>",
            abstract_content: "Semantic communication, leveraging advanced deep learning techniques, emerges as a new paradigm that meets the requirements of next-generation wireless networks. However, current semantic communication systems, which employ neural coding for feature extraction from raw data, have not adequately addressed the fundamental question: Is general feature extraction through deep neural networks sufficient for understanding semantic meaning within raw data in semantic communication? This article is thus motivated to clarify two critical aspects: semantic understanding and general semantic representation. This article presents a standardized definition on semantic coding, an extensive neural coding scheme for general semantic representation that clearly represents underlying data semantics based on contextual modeling. With these general semantic representations obtained, both human- and machine-centric end-to-end data transmission can be achieved through only minimal specialized modifications, such as fine-tuning and regularization. This article contributes to establishing a commonsense that semantic communication extends far beyond mere feature transmission, focusing instead on conveying compact semantic representations through context-aware coding schemes.",
            motivation: "<p>This article is motivated by a fundamental limitation we observed in current semantic communication systems. While these systems effectively employ deep neural networks for feature extraction—a process we term general neural coding—they do not adequately address the core challenge of true semantic understanding. <b>The features extracted are often statistically sound but semantically redundant, failing to capture the conceptual equivalence between different but related data instances</b>. For example, a model might differentiate between two distinct dog breeds (such as \"corgi\" and \"husky\") but fail to abstract them to the higher-level, shared concept of \"dog,\" leading to inefficient and redundant representations.</p><p>This inadequacy <b>leaves two critical questions unresolved</b>: first, which features are genuinely relevant for conveying semantic meaning, as opposed to simply enabling pattern discrimination? Second, how can we extract these meaningful representations in a concise and generalized manner within a unified framework?</p><p>Therefore, this article is motivated to establish a clear distinction between mere feature transmission and true semantic communication. <b>Our work aims to introduce a standardized coding workflow, which we call semantic coding, that goes beyond simple feature extraction to focus on identifying and conveying compact, context-aware semantic representations</b>. We believe this is necessary to build a more robust and efficient foundation for the next generation of intellicise wireless networks.</p>",
            methodology_p1: "In this work, our methodology is centered on establishing a standardized coding workflow for semantic communication that moves beyond the limitations of general neural coding. The semantic communication system architecture, as illustrated in the overview in Fig. 1, delineates the end-to-end process into three distinct stages: <b>Semantic Representation</b>, <b>Feature Compression</b>, and <b>Signal Transmission</b>. While existing systems have focused heavily on the latter two stages, our primary contribution lies in formalizing the first and most fundamental stage, which we achieve through a novel process called semantic coding.",
            methodology_p2: "To appreciate the necessity of our approach, it is crucial to understand the shortcomings of general neural coding. As we compare the two in Fig. 2, general neural coding, often realized as compressive coding (<i>e.g.</i>, nonlinear transform coding), primarily focuses on optimizing the entropy of latent features for efficient compression. It takes raw, high-dimensional data and seeks to create a compressed bitstream that can be reconstructed with minimal distortion. However, this process often operates on features that, while statistically sound, are semantically redundant and lack interpretable meaning. It struggles to capture the underlying concepts within the data, leading to limitations in generalization capability, adaptive feature extraction, and contextual relationship modeling. In contrast, our proposed semantic coding serves as an essential pre-processing module. <b>Its primary goal is not entropy optimization but rather dimensionality reduction</b>; it transforms the raw data into a compact, low-dimensional set of latent representations that are semantically rich. By first mapping the complex data manifold to a more manageable semantic latent space, we make the subsequent tasks of compression and transmission far more efficient and robust.",
            methodology_p3: "<p>The core of our methodology is the context-aware three-step pipeline for semantic coding, which is illustrated in Fig. 3. This pipeline is designed to systematically extract and refine general semantic representations from raw data.</p><p>First, we employ <b>Tokenization</b>. In this step, the input data, such as an image, is processed by an attention-aided model, like a Vision Transformer (ViT), to break it down into a sequence of embedded tokens. This initial step makes the complex contextual information within the data manageable and allows the model to assign importance to different regions.</p><p>Second, and most critically, we introduce <b>Reorganization</b>. This is the key mechanism that distinguishes our work. We recognized that the initial set of tokens obtained by the tokenizer contains significant semantic overlap. For instance, multiple tokens might represent different patches of the \"sky\" in an image. Through a similarity-based consolidation process, such as bipartite soft matching, we identify and merge these semantically related tokens into a single, representative token. This step dramatically reduces the number of tokens required to represent the core concepts of the data, <i>e.g.</i>, from nearly 200 tokens down to just 10 to 30, thereby creating a truly compact and meaningful semantic latent space.</p><p>Finally, we apply an optional <b>Quantization</b> step. The resulting set of compact semantic tokens can be mapped to discrete latent codes from a learned codebook. This process makes the semantic representation even more discernible and sparse, approaching our ideal of \"one code, one concept.\" Furthermore, it enhances compatibility with auto-regressive (AR) generators at the receiver, which are often used for high-fidelity content recovery.</p>",
            methodology_p4: "In summary, our methodology defines semantic coding as this structured process of tokenization, reorganization, and quantization. It effectively decouples the process of understanding meaning from the recovery of transmitted data, creating a compact and general semantic representation that serves as a versatile foundation for any subsequent communication task.",
            framework_p1: "Our technical framework, depicted in Fig. 4, outlines how we implement semantic coding to enable efficient and effective semantic communication. We conceptualize the entire process as a three-stage workflow, moving from data-oriented semantic encoding to compressive and transmissive coding, and finally to task-oriented specific semantic decoding.",
            framework_p2: "<p>At the very beginning, we have the <b>Data-Oriented General Semantic Encoding</b> stage. This is where the raw data, whether it's an image, text, or any other form, undergoes our proposed semantic coding process. We start with a <b>Tokenizer</b>, which takes the raw data and transforms it into a set of embedded tokens. Think of these tokens as elementary units capturing different aspects of the data. Then comes the crucial <b>Reorganization</b> step. Here, we group and merge similar tokens based on lightweight search algorithms such as bipartite soft matching. This is not just about compression; it's about consolidating redundant information and distilling the data into a far smaller, yet semantically rich, set of \"semantic tokens.\" For instance, if many tokens represent different shades of blue in the sky, we might merge them into a single token representing \"sky.\" Finally, we have an optional <b>Quantization</b> step, where these continuous semantic tokens can be converted into discrete codes. This makes them even more compact and easier for machines to process, essentially turning abstract meanings into distinct, manageable units. The key here is to capture the general semantics of the data, independent of any specific task or user preference.</p><p>Once we have these compact semantic tokens, they move into the <b>Compressive and Transmissive Encoding/Decoding</b> stage. Here, the semantic tokens are further processed to optimize for transmission over a wireless channel. This involves standard techniques like joint source-channel coding (JSCC), which efficiently bundles the semantic information into a signal that can be transmitted. On the receiver side, the inverse process occurs, where the signal is decoded back into semantic tokens, ready for interpretation.</p><p>The final stage is <b>Task-Oriented Specific Semantic Decoding</b>. This is where the power of our framework truly shines. Because our semantic encoding creates a general semantic representation, the receiver can then tailor the decoding process to specific tasks or user intents. For instance, if the goal is high-fidelity reconstruction of an image, we can employ a <b>Generator</b> to meticulously recreate the original content from the semantic tokens. However, if the task is recognition or control, perhaps for a machine agent, we might use a <b>Discriminator</b> (like a classifier) to extract only the most relevant information for decision-making. This stage also incorporates \"<b>Prior Knowledge</b>,\" which can be user prompts or a knowledge base, to guide the interpretation and ensure the recovered content aligns with the specific needs of the human or machine. This flexible, decoupled design means that the same general semantic representation can support a wide array of applications without requiring complete re-training for each new task.</p>",
            case_p1: "To empirically validate the advantages of our proposed semantic coding framework, we conducted a case study centered on end-to-end wireless image transmission, with the results detailed in Fig. 5.",
            case_p2: "<p>For our experimental setup, we simulated a point-to-point communication system where a transmitter encodes and sends RGB images over a noisy Additive White Gaussian Noise (AWGN) channel, which is a standard model for wireless environments. We fixed the signal-to-noise ratio at a challenging 1 dB and evaluated performance across several data rates. Our proposed framework implemented semantic coding by integrating a bipartite soft matching module into an off-the-shelf Vision Transformer (ViT-B/16) model. This module performs the crucial token reorganization step, compressing images into a compact set of either 30 or 10 semantic tokens. A key advantage of our methodology, which we demonstrated here, is that this entire process <b>requires no retraining or fine-tuning of the pre-trained ViT model</b>. At the receiver, a Stable Diffusion model was used as the generator to perform content recovery from the transmitted semantic tokens. For comparison, we implemented a well-established neural coding baseline that performs deep joint source-channel coding directly on the image features without our semantic reorganization step. To evaluate performance, we used two standard metrics: <b>PSNR</b>, to measure the consistency and faithfulness of the reconstruction, and <b>FID</b>, to assess the perceptual quality and realism of the generated images.</p><p>The results of this case study demonstrate the effectiveness of our semantic coding approach. Quantitatively, our method outperforms the neural coding baseline across all tested data rates. <b>The performance gap is particularly significant at the lower rates, where bandwidth is most constrained</b>. Our method consistently achieves a higher PSNR, indicating better reconstruction fidelity, and a lower FID score, which signifies more realistic and perceptually pleasing images. Moreover, our semantic coding achieves FP16 inference at 524 images/s with 74.06% Top-1 accuracy, only 5% lower while delivering ~1.5× the throughput of vanilla ViT-B/16, surpassing neural coding.</p><p>Visually, the difference is even more striking. The reconstructed images produced by our framework are notably more realistic and semantically coherent, maintaining acceptable consistency even when using as few as 10 tokens. In contrast, the baseline neural coding approach tends to produce blurrier and more distorted outputs at similar data rates. This confirms that by focusing on compact, meaningful semantic representations, our framework <b>achieves a more favorable trade-off between realism and consistency</b>, validating its effectiveness for robust and efficient human-centric semantic communication.</p>",
            conclusion: "<p>In concluding our work, we want to emphasize that our goal was to address a fundamental issue holding back the field of semantic communication. We observed that current neural coding schemes used in semantic communication systems are good at compressing data but not at truly understanding it. They transmit features, but not meaning.</p><p>In this article, we've introduced an improved approach through our <b>standardized coding workflow</b>. At its heart is what we call <b>semantic coding</b>, a simple yet powerful three-step process: we tokenize the data, reorganize those tokens to keep only the unique concepts, and then (if needed) quantize them into discrete codes. This moves us closer to an ideal of \"one code, one concept,\" making communication far more efficient.</p><p>Looking forward, we believe our framework opens up several exciting possibilities. First, it helps alleviate the problem of <b>semantic uncertainty</b>—the fact that everyone interprets things differently. Our method separates the objective meaning of the data, which is encoded at the transmitter, from the specific interpretation, which is handled by the receiver. This creates a much clearer and more flexible system.</p><p>Second, our approach is crucial for building truly <b>resilient communication</b> systems that can work in extreme environments like deep space or underwater. By sending compact, meaningful tokens, we protect the core information itself. Even if the signal gets damaged, a smart receiver can often use these general semantic \"seeds\" to regenerate the intended content, essentially healing the communication.</p><p>Finally, this work helps catalyze a new paradigm of <b>generative communication</b>. Instead of sending massive messages, we can just transmit a handful of semantic tokens and let a generative model at the other end create the content. This is not only incredibly efficient but also opens the door to a new generation of smart applications in 6G.</p><p>In essence, our work provides a technical guideline to shift communication from simply sending features to intelligently conveying meaning, unifying how information is represented at the transmitter and how it's interpreted by the user or machine at the receiver. We hope this work will help advance standardized coding workflows for semantic communication and serve as a reference for the future development of intellicise wireless networks.</p>",
            fig1: "Fig. 1: <b>Overview of the coding workflow in semantic communication systems.</b> <i>Notations</i>: Solid single arrows represent data flow, while dashed double arrows represent evaluation. $\\mathcal{T}$ and $\\mathcal{G}$ denote semantic tokenizer and detokenizer; $\\mathcal{A}$ and $\\mathcal{S}$ represent analysis and synthesis transforms; $\\mathcal{E}$ and $\\mathcal{D}$ indicate transmissive encoding and decoding with modulation and demodulation. $\\mathcal{W}$ represents the wireless channel, $\\mathcal{Q}$ denotes quantization, and $R$ and $D$ specify rate and distortion respectively. Bold lowercase letters (e.g., $\\mathbf{x}$) denote vectors, and vectors recovered through inverse operations are represented by bold lowercase letters with hats (e.g., $\\hat{\\mathbf{x}}$). Unless otherwise specified, all $\\mathbf{x}$ in this article represents raw data, $\\mathbf{z}$ represents semantic latent representations, $\\mathbf{y}$ represents neural latent features obtained through nonlinear transforms, and after quantization and entropy coding, these latent features are further compressed into bit streams $\\mathbf{q}$, which are finally converted into signal sequences $\\mathbf{s}$ that can be transmitted over wireless channels through transmissive coding (e.g., joint source-channel coding) and modulation. In general, semantic coding reduces dimensionality of raw data, compressive coding optimizes entropy of latent features, and transmissive coding enables symbol-to-signal conversion for transmission.",
            fig2: "Fig. 2: <b>Semantic coding for compact representation (towards dimensionality reduction) vs. Compressive coding for feature compression (towards entropy optimization).</b> <i>Notations</i>: $\\mathrm{dim}(\\cdot)$ specify data dimensionality, and $H(\\cdot)$ define information entropy. Bold lowercase letters with tildes denote quantized vectors (e.g., $\\tilde{\\mathbf{z}}$). The dashed box represents optional operations. Compressive coding implements data compression through three components: (1) an encoder performing analysis transform $\\mathcal{A}$ to convert data into a compression-friendly form, (2) a decoder executing synthesis transform $\\mathcal{S}$ as the inverse operation, and (3) an entropy model predicting the probability distribution of quantized latent features in both encoding and decoding modules ($p_{\\mathbf{\\theta}}(\\tilde{\\mathbf{y}}) \\to p(\\mathbf{y})$ with model parameter $\\mathbf{\\theta}$, achieving optimized entropy $H(\\tilde{\\mathbf{y}}) \\ll H(\\mathbf{z})$). These modules, typically implemented with DNNs, are trained jointly end-to-end. For high-dimensional real-world data with unknown manifolds, entropy coding for feature compression becomes challenging. Semantic coding addresses this by serving as a pre-processing module for dimensionality reduction, obtaining low-dimensional latent representations ($\\mathrm{dim}(\\mathbf{z}) \\ll \\mathrm{dim}(\\mathbf{x})$, either continuous or discrete, whose distributions are easier to estimate) before compressive coding. The tokenizer $\\mathcal{T}$ extracts general semantic representations independent of individual preferences, while the detokenizer $\\mathcal{G}$ adapts to specific tasks.",
            fig3: "Fig. 3: <b>Illustration of extracting general semantic representations.</b> Raw data in <i>source space</i> is transformed into tokens via tokenization, and these obtained tokens in <i>token embedding space</i> are then reorganized into compact semantic tokens constituting the so-called <i>semantic latent space</i> through similarity-based consolidation. In this process, similar tokens are aggregated together, indicating high semantic relevance. Optionally, scalar or vector quantization can be applied to represent semantic tokens as latent codes retrieved from a codebook. Furthermore, synonymous concepts could be further aggregated within conceptual contexts for more generalized semantic understanding. This means sub-concepts can be further reorganized in the semantic latent space by aggregating similar ones through a process analogous to token reorganization. Subsequent quantization would yield sparse codes that reduce semantic overlap and ambiguity, approaching \"one code one concept\".",
            fig4: "Fig. 4: <b>The technical workflow of semantic coding enabled semantic communication.</b> <i>Notations:</i> $\\mathrm{JSCE}$ and $\\mathrm{JSCD}$ denote joint source-channel encoding and decoding respectively. They constitute joint source-channel coding as an implementation of transmissive coding for real-time high-dimensional data transmission. Dashed single arrows represent optional generator or discriminator based on task levels. General semantic representations are extracted through semantic encoding via three steps: tokenization, reorganization (for compact semantic tokens, with deployment ease and computational efficiency), and quantization with the optionality depending on the properties of utilized semantic detokenizers. To be specific, token reorganization is achieved through three steps: (1) group all tokens into two sets $\\mathrm{A}$ and $\\mathrm{B}$ of roughly equal size, (2) draw edges from each token in $\\mathrm{A}$ to match its most similar token in $\\mathrm{B}$ using self-attention and retain only the $r$ most similar edges (we set $r=8$), and (3) merge connected tokens to form new token set $\\mathrm{C}$ with consistent semantic meaning. Subsequently, compressive and transmissive coding (e.g., nonlinear transform source-channel coding) are employed for end-to-end wireless transmission. At receiver side, semantic decoding is tailored to specific human intents or machine tasks. For low-level tasks focusing on detail reconstruction, generators (e.g., generative models) are employed, while for high-level tasks emphasizing recognition and decision-making, discriminators (e.g., learned classifiers) are typically utilized. The intended content is recovered from general semantic representations conditioned on guidance from specialized inputs (e.g., user prompts as prior knowledge).",
            fig5: "Fig. 5: <b>Results for end-to-end wireless image transmission with semantic coding.</b> \" $\\mathrm{Recon.}$\", \" $\\mathrm{Neu.\\ Cod.}$\", \" $\\mathrm{Sem.\\ Cod.}$\" and \"w/\" denote \"Reconstructions\", \"Neural Coding\", \"Semantic Coding\" and \"with\" respectively. A higher PSNR or lower FID score indicates better performance. Features are captured directly from neural coding baseline. The reconstructed results show semantic coding achieves realistic images while maintaining acceptable consistency. Zoom in for details.",
            placeholder: "Waiting to write down",
            footer_text: "&copy; 2025 Hai-Long Qin. All rights reserved. | Last updated: August 2025",
            bib_title: "BibTeX Citation",
            bib_copy: "Copy to Clipboard",
            bib_copied: "Copied!"
        },
        zh: {
        // pageTitle: "神经编码并非总具语义性",
        paperTitle: "神经编码并非总具语义性：面向语义通信标准化编码工作流的研究",
        author_1: "秦海龙<sup>1</sup>",
        author_2: "戴金晟<sup>1,†</sup>",
        author_3: "王思贤<sup>1</sup>",
        author_4: "秦晓琦<sup>1</sup>",
        author_5: "邵硕<sup>2</sup>",
        author_6: "牛凯<sup>1</sup>",
        author_7: "许文俊<sup>1</sup>",
        author_8: "张平<sup>1,†</sup>",
        affiliation1: "<sup>1</sup>北京邮电大学",
        affiliation2: "<sup>2</sup>上海理工大学",
        affiliation_corr: "<sup>†</sup>通讯作者",
        venueInfo: "已录用于 IEEE Communications Standards Magazine (COMSTD), 2025年",
        btn_paper: "论文",
        tooltip_cite: "引用本文",
        tooltip_lang: "切换语言",
        nav_tldr: "要点概述",
        nav_abstract: "摘要",
        nav_overview: "研究动机",
        nav_methodology: "核心方法",
        nav_framework: "技术框架",
        nav_case_study: "实验案例",
        nav_conclusion: "结论",
        tldr_content: "<p>(1) 我们为语义通信构建了一套<b>标准化的编码工作流</b>，其核心不再是传输神经网络直接提取的深度特征，而是传递感知上下文信息的紧凑<b>语义表征</b>。该工作流将语义编码的实现过程规范为三个步骤：<b>tokenization</b>、<b>reorganization</b> 与可选的 <b>quantization</b>。</p><p>(2) 我们提出 <b>token reorganization</b> 这一核心机制，它能够依据上下文间的相关性，自适应地识别并融合语义相似的 token。此举旨在将原始数据压缩为一组极简的<b>概念基元</b>，在 quantization 的加持下，实现“<b>一个码字即可表示一种概念 (one code, one concept)</b>”的理想效果，从而显著消除发送信源存在的信息冗余，并简化数据分布的估计与建模过程。</p><p>(3) 无线图像传输的仿真实验证明，该工作流显著提升了通信的有效性与鲁棒性，仅需 <b>10-30个语义 token</b>，其重建图像的逼真度 (FID更低) 与保真度 (PSNR更高) 便远超传统的神经编码方案，此优势在极低码率下尤为明显。</p><p>(4) 我们设计了一种灵活的<b>解耦式编解码架构</b>，将发送端的通用语义编码与接收端的特定任务解码从功能上分离。这意味着同一份语义表征可服务于多样化的应用：既能为人类用户重建高保真内容，也能为机器智能体提供高效的决策依据。接收端只需根据特定的任务需求向解码过程注入条件以正则化通用语义表征，便能从中获取适配通信意图的结果。</p>",
        abstract_content: "深度学习所驱动的语义通信技术已成为下一代无线网络的重要范式。然而，当前主流的语义通信系统在使用神经编码（泛指一般的基于深度神经网络的压缩编码策略）提取数据特征时，并未充分解决一个根本性问题：一般意义上的深度特征提取是否意味着真正的语义理解？为厘清此问题，本文旨在探讨语义理解与通用语义表征两大核心议题。我们提出了语义编码的标准化定义，并将其作为一种进阶版的神经编码方案，它基于上下文建模来精确表征数据的内在语义。基于获取的通用语义表征，系统仅需对深度学习模型进行微调或正则化等些许调整，便可有效地支持面向人类（human-centric）和面向机器（machine-centric）的各类端到端传输任务。本文旨在推动学界形成一种共识：语义通信的发展，应超越单纯的特征传输，转向通过基于上下文感知的编码方案，实现极简语义表征的智能传递。",
        motivation: "<p>本文的研究动机，源于我们对当前语义通信系统所存在的局限性的观察。尽管这些系统能通过深度神经网络有效提取特征——此过程常被称为神经编码——但它们无法确保特征提取过程实现了真正的语义理解。<b>其提取的特征尽管在统计层面表现良好，但在语义层面却存在大量冗余，未能捕捉不同数据实例间等价的高层次抽象概念。</b>例如，一个模型能区分不同品种的狗（比如“柯基”和“哈士奇”），却无法将它们抽象为“狗”这一共享概念，这势必导致表征的低效与冗余。</p><p>这种局限性<b>引出了两个悬而未决的核心问题</b>：第一，如何评判哪些特征真正承载了特定的语义信息？第二，如何以简单通用的方式来获取语义表征？</p><p>因此，我们工作的核心在于建立一套标准化的编码工作流，我们称之为“语义编码 (semantic coding)”。<b>它旨在利用上下文建模方法获取数据的极简语义表征，而非仅仅是一般意义上的深度特征提取</b>。我们认为，这对于构建下一代智简（intellicise）无线网络所需的鲁棒高效的基础编码框架是十分必要的。</p>",
        methodology_p1: "在本文中，我们的核心方法是为语义通信建立一套标准化的编码工作流，以突破传统神经编码的瓶颈。如图1所示，此语义通信系统架构将端到端流程显式地划分为三个阶段：<b>语义表征 (Semantic Representation)</b>、<b>特征压缩 (Feature Compression)</b> 与 <b>信号传输 (Signal Transmission)</b>。现有工作大多聚焦于后两个阶段，而我们的核心贡献在于对第一个、也是最基础的阶段进行实现方式上的标准化，并通过我们称之为“语义编码”的流程加以实现。",
        methodology_p2: "要理解我们方法的必要性，首先需了解神经编码的不足。如图2对比所示，神经编码在本质上是一种压缩编码（如非线性变换编码），其目标是通过优化隐式特征的熵来高效压缩数据。它能将高维原始数据转化为可重建的压缩码流，但此过程产生的特征虽在统计上有效，在语义层面却往往是冗余且难以解释的，更无法捕捉数据背后的深层概念，这限制了其泛化能力与上下文关系建模能力。相比之下，我们提出的语义编码则是一个关键的预处理模块。<b>其首要目标并非追求极致的熵压缩率，而是实现有效的数据降维</b>；它将高维的原始数据，映射到一个维度更低、语义更凝练集中的隐空间中。通过将复杂的数据流形首先投影到更易于处理的语义隐空间，后续的压缩和传输任务将变得更为鲁棒高效。",
        methodology_p3: "<p>语义编码的核心，是一套如图3所示、具备上下文感知能力的三步式流水线，它旨在从原始数据中系统地提炼出通用语义表征。</p><p>第一步是 <b>Tokenization</b>。输入数据（如图像）首先由一个如Vision Transformer (ViT) 的注意力模型处理，被分解为一系列 embedded tokens。此步骤将复杂数据转化为易于管理的、蕴含了区域重要性信息的 token 序列。</p><p>第二步，也是最关键的创新点，是 <b>Reorganization</b>。我们发现，tokenizer 产生的初始 token 集合存在明显的语义重叠（例如，图像中多处表示“天空”的色块）。通过基于相似度的合并算法（如二分图软匹配），我们将这些语义相关的 token 聚合成一个更具代表性的新 token。在本文的例子中，这一步能将代表核心概念的 token 数量从近200个缩减至10到30个，从而构建出一个真正紧凑且意义明确的语义隐空间。</p><p>最后一步是可选的 <b>Quantization</b>。经过 Reorganization 的极简语义 token 可被进一步映射到一个预学习码本中的离散码字。这个过程使语义表征更加清晰、稀疏，以至于达到“一码一概念”的理想状态，同时也增强了与接收端用于高保真内容重建的自回归 (AR) 生成器的兼容性（如绝大多数AI大模型）。</p>",
        methodology_p4: "简言之，我们的方法将语义编码定义为由 tokenization、reorganization 和 quantization 构成的结构化流程。它将发送端的“语义理解”与接收端的“内容恢复”这两个过程解耦，通过构造紧凑的通用语义表征，为下游通信任务提供了灵活泛用的解码基础。",
        framework_p1: "我们的技术框架（如图4所示）展示了如何通过语义编码实现鲁棒高效的语义通信。我们将全过程设计为一个三阶段的工作流：从面向数据的通用语义编码，到压缩传输编码，最终到面向任务的专用语义解码。",
        framework_p2: "<p>流程始于<b>面向数据的通用语义编码 (Data-Oriented General Semantic Encoding)</b>。在这一阶段，原始数据（图像、文本等）经过我们提出的语义编码处理。首先，<b>Tokenizer</b> 将数据转换为基础的 embedded tokens。随后，关键的 <b>Reorganization</b> 步骤利用轻量级搜索算法（如二分图软匹配）对相似的 token 进行分组与合并。此举的核心目的并非简单压缩，而是提炼与整合，将冗余信息凝聚成一套数量极少但语义丰富的“语义 token”（例如，将代表天空中不同蓝色的多个 token 融合成一个“天空” token）。最后，可选的 <b>Quantization</b> 步骤可将连续的语义 token 转换为离散码字，使其更紧凑、更易于机器处理，将抽象意义转化为明确的管理单元。本阶段的关键在于，所生成的通用语义表征独立于任何具体任务或用户偏好。</p><p>获得紧凑的语义 token 后，流程进入<b>压缩与传输编解码 (Compressive and Transmissive Encoding/Decoding)</b> 阶段。在这里，语义 token 将被进一步处理以适应无线信道传输，例如采用信源信道联合编码（JSCC）等技术，将其高效地封装成物理信号。在接收端，则执行逆过程，将信号解码恢复为语义 token。</p><p>流程的终点是<b>面向任务的专用语义解码 (Task-Oriented Specific Semantic Decoding)</b>，这也集中体现了本框架的优势。由于我们获得的是一套通用的语义表征（可理解为各向同性的语义“种子”），接收端可以根据具体的用户意图或任务需求，灵活地定制解码过程。例如，对于高保真图像重建任务，可调用<b>生成器 (Generator)</b> 精细地复原内容；而对于机器智能体的识别或控制任务，则可使用<b>判别器 (Discriminator)</b>（如分类器）仅提取决策所需的关键信息。该阶段还可以融合用户指令 (prompts) 或知识库等“<b>先验知识 (Prior Knowledge)</b>”来引导解码，确保恢复的内容精准符合特定需求。这种灵活的解耦设计，意味着同一套通用语义表征能够支撑广泛的下游应用，而无需为每个新任务重新训练整个模型。",
        case_p1: "为验证我们所提出语义编码框架的实际效能，我们以端到端无线图像传输为仿真场景，进行了一项案例研究，结果详见图5。",
        case_p2: "<p>实验中，我们模拟了一个点对点通信系统，发射机在标准的加性高斯白噪声（AWGN）信道上发送RGB图像，并将信噪比固定在颇具挑战性的1 dB。我们的框架将一个二分图软匹配模块集成于一个现成的Vision Transformer（ViT-B/16）模型中，以执行核心的 token reorganization 操作，最终将图像压缩为10或30个语义 token。此方法的一大优点在于：整个编码过程<b>无需对预训练的ViT模型进行任何重新训练或微调</b>。在接收端，我们采用Stable Diffusion模型作为生成器，负责从接收到的语义 token 中重建图像。作为对比，我们选用了一个常用的神经编码基线，该基线直接对图像特征进行深度信源信道联合编码 (DeepJSCC)。评价指标采用<b>PSNR</b>（衡量重建保真度）与<b>FID</b>（衡量视觉逼真度）。</p><p>实验结果证明了我们方法的有效性。量化分析显示，在所有测试码率下，我们的方法均表现出优于基线方案的性能。<b>尤其在码率极为有限的场景下，这一性能优势尤为明显</b>。我们的方法在不同码率下均能获得更高的PSNR和更低的FID，意味着重建的图像既保真又真实。此外，我们的语义编码在FP16精度下推理速度达到524张/秒，Top-1准确率达74.06%，吞吐量是原始ViT-B/16的约1.5倍，性能优于神经编码方案。</p><p>从视觉效果上看，二者差异显著。我们的框架重建的图像语义连贯、细节逼真，即使仅用10个 token 也能保持较高的视觉质量。相比之下，基线方法在同等码率下得到的图像则明显模糊且失真。这证实了，通过专注于提炼紧凑而有意义的语义表征，我们的框架在<b>逼真度与一致性之间取得了更好的平衡</b>，展现了其在构建鲁棒高效的下一代语义通信系统中的应用潜力。</p>",
        conclusion: "<p>总而言之，本文旨在解决一个长期制约语义通信发展的根本性问题：当前主流的神经编码方案，在本质上传输的是冗余的深度特征，而非凝练的语义概念。</p><p>为此，我们引入了一套<b>标准化的编码工作流</b>，其核心是“<b>语义编码 (semantic coding)</b>”这一简单高效的三步流程：tokenization, reorganization, 和 quantization。该流程旨在趋近“一码一概念”的理想目标，从根本上提升通信效率。</p><p>展望未来，本框架揭示了若干值得探索的研究方向。首先，它为解决<b>语义不确定性</b>问题提供了新思路。通过将客观意义的表征（在发送端完成）与主观意图的解释（在接收端处理）相解耦，系统变得更为清晰且灵活。</p><p>其次，对于深空、水下等极端环境下的<b>高韧性通信</b>，我们的方法具有重要意义。通过传输高度浓缩的通用语义“种子”，即使信道严重恶化，接收机仍有可能依据这些核心概念重构出完整信息，从而实现通信的“自我愈合”。</p><p>最后，这项工作有助于催生一种新的<b>生成式通信</b>范式。未来，我们无需传输庞大的原始数据，只需传递少量语义 token，由接收端的生成模型按需创造内容。这不仅是效率的提升，更为6G时代的智能应用开启了新的可能性。</p><p>本质上，我们的工作为推动通信范式从“传输特征”向“传递意义”的根本性转变提供了技术指引，统一了信息在发送端的共性表征方式与在接收端的个性解释方式。我们希望这项工作能促进语义通信标准化编码的发展，并为未来智简无线网络的演进提供有益的参考。",
        fig1: "图1：<b>语义通信系统中的编码工作流概览。</b> <i>符号说明</i>：实线单箭头代表数据流，虚线双箭头代表指标评价。$\\mathcal{T}$ 和 $\\mathcal{G}$ 分别表示语义 tokenizer 和 detokenizer；$\\mathcal{A}$ 和 $\\mathcal{S}$ 表示解析和合成变换；$\\mathcal{E}$ 和 $\\mathcal{D}$ 表示包含调制和解调的传输编码与解码。$\\mathcal{W}$ 代表无线信道，$\\mathcal{Q}$ 表示 quantization，R 和 D 分别表示码率和失真。$\\mathbf{x}$ 代表原始数据，$\\mathbf{z}$ 代表语义隐式表征，$\\mathbf{y}$ 代表经非线性变换获得的神经隐式特征，这些隐式特征经 quantization 和熵编码后被压缩为比特流 $\\mathbf{q}$，最终通过传输编码（如信源信道联合编码）转换为信号序列 $\\mathbf{s}$。$\\hat{\\mathbf{x}}$ 等带帽符号表示重建后的对应向量。总的来说，语义编码旨在降低原始数据的维度，压缩编码旨在优化隐式特征的熵，而传输编码则实现符号到信号的转换以进行传输。",
        fig2: "图2：<b>面向极简表征的语义编码（旨在降维） vs. 面向特征压缩的压缩编码（旨在优化熵）。</b> <i>符号说明</i>：$\\mathrm{dim}(\\cdot)$ 表示数据维度，$H(\\cdot)$ 表示信息熵。$\\tilde{\\mathbf{z}}$ 等带波浪线符号表示量化后的向量。虚线框为可选操作。压缩编码通过编码器（解析变换 $\\mathcal{A}$）、解码器（合成变换 $\\mathcal{S}$）及熵模型 $p_{\\mathbf{\\theta}}(\\tilde{\\mathbf{y}})$ 联合优化，旨在最小化隐式特征 $\\mathbf{y}$ 的熵 $H(\\tilde{\\mathbf{y}})$。对于流形未知的高维数据，直接进行熵编码极具挑战性。语义编码作为预处理模块，通过将高维数据 $\\mathbf{x}$ 映射为低维隐式表征 $\\mathbf{z}$（$\\mathrm{dim}(\\mathbf{z}) \\ll \\mathrm{dim}(\\mathbf{x})$），显著简化了后续的压缩任务。其中，tokenizer $\\mathcal{T}$ 提取通用的语义表征，而 detokenizer $\\mathcal{G}$ 则面向具体任务进行特别设计。",
        fig3: "图3：<b>通用语义表征的提取过程图解。</b> <i>源空间</i> 的原始数据经 tokenization 转换为 <i>token 嵌入空间</i> 中的一系列 token。随后，通过基于相似度的合并（reorganization），这些 token 在 <i>语义隐空间</i> 中被重组为一套极简语义 token，语义高度相关的 token 在此过程中被聚合。可选地，可应用标量或向量 quantization，将语义 token 表达为码本中的离散稀疏码。更进一步，同义概念可在概念级别的上下文语境中被再次聚合（过程类似于 token reorganization），从而实现更泛化的语义理解。最终得到的稀疏码能够有效减少语义的重叠与歧义，趋近“一码一概念”的理论目标。",
        fig4: "图4：<b>语义编码赋能的语义通信技术工作流。</b> <i>符号说明：</i>$\\mathrm{JSCE}$/$\\mathrm{JSCD}$ 分别表示信源信道联合编码/解码。通用语义表征通过三步提取：tokenization, reorganization（生成极简语义 token）, 以及可选的 quantization。具体地，token reorganization 可通过以下步骤实现：（1）将所有 token 均分为两组 $\\mathrm{A}$ 和 $\\mathrm{B}$；（2）利用自注意力机制，为 $\\mathrm{A}$ 中每个 token 在 $\\mathrm{B}$ 中寻找最相似的匹配，并保留 $r$ 个最强匹配（本研究设 $r=8$）；（3）将匹配的 token 对进行合并，形成语义一致的新 token 集合 $\\mathrm{C}$。随后，压缩与传输编码（如非线性变换信源信道编码）负责端到端传输。在接收端，语义解码依据具体任务定制：低级重建任务采用生成器（Generator），而高级决策任务则采用判别器（Discriminator）。最终的输出内容是在通用语义表征的基础上，结合特定输入（如作为先验知识的用户提示）指导生成的。",
        fig5: "图5：<b>采用语义编码的端到端无线图像传输结果。</b> “ $\\mathrm{Recon.}$”、“ $\\mathrm{Neu.\\ Cod.}$”、“ $\\mathrm{Sem.\\ Cod.}$”、“w/” 分别代表“重建结果”、“神经编码”、“语义编码”和“使用”。更高的PSNR或更低的FID分数意味着更优的性能。本方案的重建结果在确保了较优一致性的前提下，实现了优于基线方法的视觉逼真度。可放大查看细节。",
        placeholder: "等待撰写",
        footer_text: "&copy; 2025 秦海龙 版权所有 | 最后更新：2025年8月",
        bib_title: "BibTeX 引用",
        bib_copy: "复制到剪贴板",
        bib_copied: "已复制!"
        }
    };
